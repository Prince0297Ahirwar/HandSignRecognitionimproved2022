{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66b714ac",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03868ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 #opencv\n",
    "import numpy as np\n",
    "import os   #helps with path\n",
    "from matplotlib import pyplot as plt #to use plt.imshow()\n",
    "import time              #to measure time between frames \n",
    "import mediapipe as mp   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf0c2ef",
   "metadata": {},
   "source": [
    "# 2 . drawingutil and hands module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc7d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpDraw=mp.solutions.drawing_utils\n",
    "mpHands=mp.solutions.hands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4d94c",
   "metadata": {},
   "source": [
    "# 3. Function to detect points on hands and then drawing on the hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9620a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB because by default opencv use bgr but we need rgb for mediapipe to process image\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "064e66e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    if results.multi_hand_landmarks:\n",
    "        for num, handsLms in enumerate(results.multi_hand_landmarks):\n",
    "            mpDraw.draw_landmarks(image,handsLms, mpHands.HAND_CONNECTIONS,\n",
    "                             mpDraw.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mpDraw.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4c5a4",
   "metadata": {},
   "source": [
    "# 4. function to get handedness ie left or right and to extract point to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300b96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(index,results):\n",
    "    label = None\n",
    "    for idx, classification in enumerate(results.multi_handedness):\n",
    "        if classification.classification[0].index == index:\n",
    "            label = classification.classification[0].label\n",
    "#         print(index,idx,label)\n",
    "    if label:\n",
    "        return label\n",
    "    else:\n",
    "        if index == 1:\n",
    "            return get_label(0,results)\n",
    "        elif index == 0:\n",
    "            return get_label(1,results)\n",
    "        else:\n",
    "            return label\n",
    "def extract_keypoints(results):\n",
    "    hands = [np.zeros(21*3),np.zeros(21*3)]\n",
    "    if results.multi_hand_landmarks:\n",
    "        for num, handsLms in enumerate(results.multi_hand_landmarks):\n",
    "            label = get_label(num,results)\n",
    "            if label == 'Right':\n",
    "                hands[0] = np.array([[res.x, res.y, res.z] for res in handsLms.landmark]).flatten()\n",
    "            if label == 'Left':\n",
    "                hands[1] = np.array([[res.x, res.y, res.z] for res in handsLms.landmark]).flatten()\n",
    "            \n",
    "    return np.concatenate(hands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb721f",
   "metadata": {},
   "source": [
    "# 5. Number of videos of dataset and videolength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50c63469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thirty videos worth of data\n",
    "no_videos = 400\n",
    "\n",
    "# Videos are going to be 15 frames in length\n",
    "video_length = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88b96f",
   "metadata": {},
   "source": [
    "# 6. defining path of dataset and also path where we will store the processed data ie folder Edata in current directory\n",
    "\n",
    "dataset link <a href=\"https://drive.google.com/drive/folders/1RZaXXy3pr7YLSv1jKFXDgmNC2nzkgV1P?usp=sharing\">DATASET<a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e7d0fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91963\\Desktop\\HandSignRecognition\\DEMO\\newmodel\\AJmodel\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ea7c852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'del',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'nothing',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'space',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = os.path.join('../../KgData/asl4g/train/')\n",
    "os.listdir(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7abcee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path for numpy array of data\n",
    "DATA_PATH = os.path.join('AtoZdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e55de01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '26.04.2022_17.30.00_REC.mp4',\n",
       " '26.04.2022_17.54.20_Latest.mp4',\n",
       " 'ATOZ',\n",
       " 'ATOZ.zip',\n",
       " 'AtoZdata',\n",
       " 'AtoZdataDistance',\n",
       " 'board1.jpg',\n",
       " 'HandGestureRecognitionNeuralNetworkA-J-v1-lstm.ipynb',\n",
       " 'HandGestureRecognitionNeuralNetworkA-J-v1.ipynb',\n",
       " 'HandGestureRecognitionNeuralNetworkA-J-v2-distance-included.ipynb',\n",
       " 'HandGestureRecognitionNeuralNetworkAllDataButSingleFrame.ipynb',\n",
       " 'LatestWorking.zip',\n",
       " 'model26April.h5',\n",
       " 'model27AprilDist.h5',\n",
       " 'model27Aprillstm.h5',\n",
       " 'model27Aprillstm2.h5',\n",
       " 'model31march.h5',\n",
       " 'New folder',\n",
       " 'runner.ipynb',\n",
       " 'trainLogsdNeuralNetworkATOZ26April',\n",
       " 'trainLogsdNeuralNetworkATOZ27AprilDist',\n",
       " 'trainLogsdNeuralNetworkATOZ27AprilLstm',\n",
       " 'trainLogsdNeuralNetworkATOZ27AprilLstm2']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0d53f",
   "metadata": {},
   "source": [
    "# 7. defining the categories of signs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e4b4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = np.array(['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','del','space','nothing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0284f1",
   "metadata": {},
   "source": [
    "# 8. making directory to store the processed data in EData folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a2e7368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories: \n",
    "    for i in range(no_videos):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH,category, str(i)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "81ebae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.walk('Edata')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd8211",
   "metadata": {},
   "source": [
    "# 9. checking data is properly fetching  and also the camera feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fb8a64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    path = os.path.join(datapath,category)\n",
    "    for imgpath in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path,imgpath))\n",
    "        img_array = cv2.resize(img_array,(480,640))\n",
    "        img_array2 = cv2.flip(img_array,1)\n",
    "        cv2.imshow(\"frame\",img_array)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "539f65a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) #creating video capture object\n",
    "## Set mediapipe model\n",
    "\n",
    "while cap.isOpened():\n",
    "    #reading feed current frame\n",
    "    ret,frame = cap.read() \n",
    "    flipframe = cv2.flip(frame,1)\n",
    "    cv2.imshow(\"Open Cv1\",flipframe) \n",
    "#     cv2.imshow('o2',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): #wait for 1 milli second to check if q is pressed on keyboard\n",
    "        break \n",
    "cap.release() #release our webcame\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd434ab",
   "metadata": {},
   "source": [
    "# 10. Extracting keypoints using extract_keypoint funtion and for every frame   #  for every category we are creating 400 videos and every video contains 15 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "213ba379",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mpHands.Hands() as model:\n",
    "    for category in categories:\n",
    "        path = os.path.join(datapath,category)\n",
    "        adl = os.listdir(path)\n",
    "        cdl = 0\n",
    "        for i in range(no_videos):\n",
    "            b = False\n",
    "            for j in range(video_length):\n",
    "                image = cv2.imread(os.path.join(path,adl[cdl]))\n",
    "#                 image = cv2.flip(image,1)\n",
    "                image = cv2.resize(image,(640,480))\n",
    "                image, results = mediapipe_detection(image, model)\n",
    "                draw_landmarks(image, results)\n",
    "#                 print(results.multi_handedness)\n",
    "                cdl += 1\n",
    "                cv2.putText(image,'f no {} Vno {} c {}'.format(j,i,category), (10,20), \n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX,0.5, (150, 0, 255), 1, cv2.LINE_AA)\n",
    "                cv2.imshow(\"frame\",image)\n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, category, str(i), str(j))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    b = True\n",
    "                    break\n",
    "            if b:\n",
    "                break\n",
    "\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b394d",
   "metadata": {},
   "source": [
    "# 11. converting categories using one hot encoding and splitting data into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9efd69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "label_map = {label:num for num, label in enumerate(categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7557db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'B': 1,\n",
       " 'C': 2,\n",
       " 'D': 3,\n",
       " 'E': 4,\n",
       " 'F': 5,\n",
       " 'G': 6,\n",
       " 'H': 7,\n",
       " 'I': 8,\n",
       " 'J': 9,\n",
       " 'K': 10,\n",
       " 'L': 11,\n",
       " 'M': 12,\n",
       " 'N': 13,\n",
       " 'O': 14,\n",
       " 'P': 15,\n",
       " 'Q': 16,\n",
       " 'R': 17,\n",
       " 'S': 18,\n",
       " 'T': 19,\n",
       " 'U': 20,\n",
       " 'V': 21,\n",
       " 'W': 22,\n",
       " 'X': 23,\n",
       " 'Y': 24,\n",
       " 'Z': 25,\n",
       " 'del': 26,\n",
       " 'space': 27,\n",
       " 'nothing': 28}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0d1cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos, labels = [], []\n",
    "for category in categories:\n",
    "    for video in range(no_videos):\n",
    "        window = []\n",
    "        for frame_num in range(video_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, category, str(video), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        videos.append(window)\n",
    "        labels.append(label_map[category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7b0af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d012d3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11600, 15, 126)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bc7e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int) #one hot encoding to convert categorial variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed72a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77262226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2320, 29)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7baaf623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9280, 15, 126)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2db0f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsamples, nx, ny = X_train.shape\n",
    "# X_train = X_train.reshape((nsamples,nx*ny))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "252c5fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsamples, nx, ny = X_test.shape\n",
    "# X_test = X_test.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c743b7e5",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b939917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bb4e1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_rf = RandomForestClassifier(random_state=42, n_jobs=-1, max_depth=5,\n",
    "                                       n_estimators=1000, oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6220e288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, n_estimators=1000, n_jobs=-1,\n",
       "                       oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a7a1b9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9280, 1890)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4487b90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9660077288941732"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_rf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "69af411b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014655172413793103"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0e48ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': [2,3,5,10,20,30,40,50,60],\n",
    "    'min_samples_leaf': [5,10,20,50,100,200,],\n",
    "    'n_estimators': [10,25,30,50,100,200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12785044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a8074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb184a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7587b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36a53d53",
   "metadata": {},
   "source": [
    "# 12. Building model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bacfabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Flatten\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34c076ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "ACCURACY_THRESHOLD = 0.95\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # print(logs.get('acc'))\n",
    "        # print(logs.get('categorical_accuracy'))\n",
    "        if(logs.get('categorical_accuracy') > ACCURACY_THRESHOLD):\n",
    "            # print(logs.get('acc'))\n",
    "            # print(logs.get('categorical_accuracy'))\n",
    "            print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(ACCURACY_THRESHOLD*100))\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Instantiate a callback object\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0e998fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('trainLogsdNeuralNetworkATOZ26Aprilimproved')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b911a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycallbacks = [callbacks,tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c69462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024,activation='relu',input_shape=(15,126)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(29, activation='softmax'))\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "39efc4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  2/290 [..............................] - ETA: 27s - loss: 3.3769 - categorical_accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0058s vs `on_train_batch_end` time: 0.1797s). Check your callbacks.\n",
      "290/290 [==============================] - 5s 16ms/step - loss: 2.1595 - categorical_accuracy: 0.3152\n",
      "Epoch 2/500\n",
      "290/290 [==============================] - 4s 15ms/step - loss: 1.0434 - categorical_accuracy: 0.6571\n",
      "Epoch 3/500\n",
      "290/290 [==============================] - 4s 15ms/step - loss: 0.8437 - categorical_accuracy: 0.7238\n",
      "Epoch 4/500\n",
      "290/290 [==============================] - 4s 15ms/step - loss: 0.7162 - categorical_accuracy: 0.7678\n",
      "Epoch 5/500\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.7075 - categorical_accuracy: 0.7748\n",
      "Epoch 6/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.6423 - categorical_accuracy: 0.8012\n",
      "Epoch 7/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.6145 - categorical_accuracy: 0.8115\n",
      "Epoch 8/500\n",
      "290/290 [==============================] - 3s 12ms/step - loss: 0.5755 - categorical_accuracy: 0.8211\n",
      "Epoch 9/500\n",
      "290/290 [==============================] - 3s 12ms/step - loss: 0.5806 - categorical_accuracy: 0.8304\n",
      "Epoch 10/500\n",
      "290/290 [==============================] - 3s 11ms/step - loss: 0.5207 - categorical_accuracy: 0.8472\n",
      "Epoch 11/500\n",
      "290/290 [==============================] - 3s 12ms/step - loss: 0.5430 - categorical_accuracy: 0.8429\n",
      "Epoch 12/500\n",
      "290/290 [==============================] - 3s 10ms/step - loss: 0.5049 - categorical_accuracy: 0.8539\n",
      "Epoch 13/500\n",
      "290/290 [==============================] - 3s 10ms/step - loss: 0.5093 - categorical_accuracy: 0.8577\n",
      "Epoch 14/500\n",
      "290/290 [==============================] - 3s 9ms/step - loss: 0.4778 - categorical_accuracy: 0.8665\n",
      "Epoch 15/500\n",
      "290/290 [==============================] - 3s 11ms/step - loss: 0.4209 - categorical_accuracy: 0.8828\n",
      "Epoch 16/500\n",
      "290/290 [==============================] - 3s 12ms/step - loss: 0.4389 - categorical_accuracy: 0.8815\n",
      "Epoch 17/500\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.4379 - categorical_accuracy: 0.8804\n",
      "Epoch 18/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.4442 - categorical_accuracy: 0.8798\n",
      "Epoch 19/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3954 - categorical_accuracy: 0.8925\n",
      "Epoch 20/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.4199 - categorical_accuracy: 0.8931\n",
      "Epoch 21/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3867 - categorical_accuracy: 0.8980\n",
      "Epoch 22/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.4002 - categorical_accuracy: 0.8941\n",
      "Epoch 23/500\n",
      "290/290 [==============================] - 3s 12ms/step - loss: 0.4119 - categorical_accuracy: 0.8921\n",
      "Epoch 24/500\n",
      "290/290 [==============================] - 3s 11ms/step - loss: 0.3928 - categorical_accuracy: 0.8980\n",
      "Epoch 25/500\n",
      "290/290 [==============================] - 3s 11ms/step - loss: 0.3852 - categorical_accuracy: 0.9005\n",
      "Epoch 26/500\n",
      "290/290 [==============================] - 3s 11ms/step - loss: 0.3966 - categorical_accuracy: 0.8971\n",
      "Epoch 27/500\n",
      "290/290 [==============================] - 3s 11ms/step - loss: 0.3704 - categorical_accuracy: 0.9032\n",
      "Epoch 28/500\n",
      "290/290 [==============================] - 3s 11ms/step - loss: 0.4350 - categorical_accuracy: 0.8918\n",
      "Epoch 29/500\n",
      "290/290 [==============================] - 3s 11ms/step - loss: 0.3451 - categorical_accuracy: 0.9106\n",
      "Epoch 30/500\n",
      "290/290 [==============================] - 3s 11ms/step - loss: 0.3711 - categorical_accuracy: 0.9059\n",
      "Epoch 31/500\n",
      "290/290 [==============================] - 3s 12ms/step - loss: 0.3445 - categorical_accuracy: 0.9144\n",
      "Epoch 32/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3333 - categorical_accuracy: 0.9165\n",
      "Epoch 33/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3789 - categorical_accuracy: 0.9045\n",
      "Epoch 34/500\n",
      "290/290 [==============================] - 3s 12ms/step - loss: 0.3078 - categorical_accuracy: 0.9227\n",
      "Epoch 35/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3581 - categorical_accuracy: 0.9105\n",
      "Epoch 36/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3509 - categorical_accuracy: 0.9126\n",
      "Epoch 37/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3444 - categorical_accuracy: 0.9140\n",
      "Epoch 38/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3463 - categorical_accuracy: 0.9142\n",
      "Epoch 39/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3562 - categorical_accuracy: 0.9103\n",
      "Epoch 40/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3120 - categorical_accuracy: 0.9247\n",
      "Epoch 41/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3274 - categorical_accuracy: 0.9203 0s - loss: 0.3334 - categorica\n",
      "Epoch 42/500\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.3346 - categorical_accuracy: 0.9155\n",
      "Epoch 43/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3326 - categorical_accuracy: 0.9211\n",
      "Epoch 44/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3195 - categorical_accuracy: 0.9204\n",
      "Epoch 45/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3232 - categorical_accuracy: 0.9226\n",
      "Epoch 46/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3599 - categorical_accuracy: 0.9105\n",
      "Epoch 47/500\n",
      "290/290 [==============================] - 3s 12ms/step - loss: 0.3274 - categorical_accuracy: 0.9195\n",
      "Epoch 48/500\n",
      "290/290 [==============================] - 3s 12ms/step - loss: 0.2997 - categorical_accuracy: 0.9242\n",
      "Epoch 49/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2980 - categorical_accuracy: 0.9244\n",
      "Epoch 50/500\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.3658 - categorical_accuracy: 0.9139\n",
      "Epoch 51/500\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.3237 - categorical_accuracy: 0.9204\n",
      "Epoch 52/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3115 - categorical_accuracy: 0.9231\n",
      "Epoch 53/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2889 - categorical_accuracy: 0.9282\n",
      "Epoch 54/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3264 - categorical_accuracy: 0.9204\n",
      "Epoch 55/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3017 - categorical_accuracy: 0.9260\n",
      "Epoch 56/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2908 - categorical_accuracy: 0.9287\n",
      "Epoch 57/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3218 - categorical_accuracy: 0.9230\n",
      "Epoch 58/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3570 - categorical_accuracy: 0.9131\n",
      "Epoch 59/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2531 - categorical_accuracy: 0.9390\n",
      "Epoch 60/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2822 - categorical_accuracy: 0.9322\n",
      "Epoch 61/500\n",
      "290/290 [==============================] - 3s 12ms/step - loss: 0.3310 - categorical_accuracy: 0.9197\n",
      "Epoch 62/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3077 - categorical_accuracy: 0.9289\n",
      "Epoch 63/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2845 - categorical_accuracy: 0.9283\n",
      "Epoch 64/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2934 - categorical_accuracy: 0.9279\n",
      "Epoch 65/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3197 - categorical_accuracy: 0.9233\n",
      "Epoch 66/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3277 - categorical_accuracy: 0.9211\n",
      "Epoch 67/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2760 - categorical_accuracy: 0.9343\n",
      "Epoch 68/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3062 - categorical_accuracy: 0.9260\n",
      "Epoch 69/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2854 - categorical_accuracy: 0.9308\n",
      "Epoch 70/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2997 - categorical_accuracy: 0.9287\n",
      "Epoch 71/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3072 - categorical_accuracy: 0.9239 0s - loss: 0.3127 - categorical_accu\n",
      "Epoch 72/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2838 - categorical_accuracy: 0.9331\n",
      "Epoch 73/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2662 - categorical_accuracy: 0.9358\n",
      "Epoch 74/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3144 - categorical_accuracy: 0.9213\n",
      "Epoch 75/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3198 - categorical_accuracy: 0.9249\n",
      "Epoch 76/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2629 - categorical_accuracy: 0.9376\n",
      "Epoch 77/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2540 - categorical_accuracy: 0.9397\n",
      "Epoch 78/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3608 - categorical_accuracy: 0.9159\n",
      "Epoch 79/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2730 - categorical_accuracy: 0.9346\n",
      "Epoch 80/500\n",
      "290/290 [==============================] - 3s 12ms/step - loss: 0.3454 - categorical_accuracy: 0.9162\n",
      "Epoch 81/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2618 - categorical_accuracy: 0.9374\n",
      "Epoch 82/500\n",
      "290/290 [==============================] - 3s 12ms/step - loss: 0.2658 - categorical_accuracy: 0.9372\n",
      "Epoch 83/500\n",
      "290/290 [==============================] - 3s 11ms/step - loss: 0.3453 - categorical_accuracy: 0.9193\n",
      "Epoch 84/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2349 - categorical_accuracy: 0.9457\n",
      "Epoch 85/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2308 - categorical_accuracy: 0.9453\n",
      "Epoch 86/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3328 - categorical_accuracy: 0.9212\n",
      "Epoch 87/500\n",
      "290/290 [==============================] - 3s 12ms/step - loss: 0.3020 - categorical_accuracy: 0.9308\n",
      "Epoch 88/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2727 - categorical_accuracy: 0.9344\n",
      "Epoch 89/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2693 - categorical_accuracy: 0.9383\n",
      "Epoch 90/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2758 - categorical_accuracy: 0.9321\n",
      "Epoch 91/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2486 - categorical_accuracy: 0.9405\n",
      "Epoch 92/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.3443 - categorical_accuracy: 0.9203\n",
      "Epoch 93/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2605 - categorical_accuracy: 0.9357\n",
      "Epoch 94/500\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.2767 - categorical_accuracy: 0.9337\n",
      "Epoch 95/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2329 - categorical_accuracy: 0.9452 0s - loss: 0.2347 \n",
      "Epoch 96/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2281 - categorical_accuracy: 0.9463\n",
      "Epoch 97/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3266 - categorical_accuracy: 0.9231\n",
      "Epoch 98/500\n",
      "290/290 [==============================] - 4s 15ms/step - loss: 0.2551 - categorical_accuracy: 0.9391\n",
      "Epoch 99/500\n",
      "290/290 [==============================] - 6s 21ms/step - loss: 0.3073 - categorical_accuracy: 0.9279\n",
      "Epoch 100/500\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 0.2807 - categorical_accuracy: 0.9342\n",
      "Epoch 101/500\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 0.2788 - categorical_accuracy: 0.9319\n",
      "Epoch 102/500\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 0.2417 - categorical_accuracy: 0.9425\n",
      "Epoch 103/500\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 0.3553 - categorical_accuracy: 0.9191\n",
      "Epoch 104/500\n",
      "290/290 [==============================] - 5s 18ms/step - loss: 0.2607 - categorical_accuracy: 0.9377\n",
      "Epoch 105/500\n",
      "290/290 [==============================] - 5s 16ms/step - loss: 0.2447 - categorical_accuracy: 0.9422\n",
      "Epoch 106/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.2349 - categorical_accuracy: 0.9443\n",
      "Epoch 107/500\n",
      "290/290 [==============================] - 5s 18ms/step - loss: 0.3560 - categorical_accuracy: 0.9179\n",
      "Epoch 108/500\n",
      "290/290 [==============================] - 5s 16ms/step - loss: 0.2637 - categorical_accuracy: 0.9371\n",
      "Epoch 109/500\n",
      "290/290 [==============================] - 5s 16ms/step - loss: 0.2307 - categorical_accuracy: 0.9457\n",
      "Epoch 110/500\n",
      "290/290 [==============================] - 4s 15ms/step - loss: 0.3686 - categorical_accuracy: 0.9144\n",
      "Epoch 111/500\n",
      "290/290 [==============================] - 4s 15ms/step - loss: 0.2924 - categorical_accuracy: 0.9351\n",
      "Epoch 112/500\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.2771 - categorical_accuracy: 0.9350\n",
      "Epoch 113/500\n",
      "290/290 [==============================] - 5s 16ms/step - loss: 0.2790 - categorical_accuracy: 0.9338\n",
      "Epoch 114/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.2624 - categorical_accuracy: 0.9380\n",
      "Epoch 115/500\n",
      "290/290 [==============================] - 4s 15ms/step - loss: 0.2736 - categorical_accuracy: 0.9341\n",
      "Epoch 116/500\n",
      "290/290 [==============================] - 5s 19ms/step - loss: 0.2305 - categorical_accuracy: 0.9460\n",
      "Epoch 117/500\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 0.2272 - categorical_accuracy: 0.9463\n",
      "Epoch 118/500\n",
      "290/290 [==============================] - 6s 21ms/step - loss: 0.3365 - categorical_accuracy: 0.9221\n",
      "Epoch 119/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.3140 - categorical_accuracy: 0.9232\n",
      "Epoch 120/500\n",
      "290/290 [==============================] - 5s 16ms/step - loss: 0.2688 - categorical_accuracy: 0.9344\n",
      "Epoch 121/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.2918 - categorical_accuracy: 0.9323\n",
      "Epoch 122/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.2498 - categorical_accuracy: 0.9404 0s - loss: 0.2498 - categorical_accuracy: 0.\n",
      "Epoch 123/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.2769 - categorical_accuracy: 0.9324\n",
      "Epoch 124/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.2615 - categorical_accuracy: 0.9394\n",
      "Epoch 125/500\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 0.2832 - categorical_accuracy: 0.9325\n",
      "Epoch 126/500\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 0.2843 - categorical_accuracy: 0.9357\n",
      "Epoch 127/500\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 0.2733 - categorical_accuracy: 0.9365\n",
      "Epoch 128/500\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 0.2410 - categorical_accuracy: 0.9433\n",
      "Epoch 129/500\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 0.3088 - categorical_accuracy: 0.9281\n",
      "Epoch 130/500\n",
      "290/290 [==============================] - 6s 19ms/step - loss: 0.2336 - categorical_accuracy: 0.9446\n",
      "Epoch 131/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.2341 - categorical_accuracy: 0.9441\n",
      "Epoch 132/500\n",
      "290/290 [==============================] - 6s 21ms/step - loss: 0.2670 - categorical_accuracy: 0.9404\n",
      "Epoch 133/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.3117 - categorical_accuracy: 0.9218\n",
      "Epoch 134/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.2511 - categorical_accuracy: 0.9397\n",
      "Epoch 135/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.2411 - categorical_accuracy: 0.9418\n",
      "Epoch 136/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.3063 - categorical_accuracy: 0.9281\n",
      "Epoch 137/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.2977 - categorical_accuracy: 0.9281\n",
      "Epoch 138/500\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.2398 - categorical_accuracy: 0.9443\n",
      "Epoch 139/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - 4s 14ms/step - loss: 0.2857 - categorical_accuracy: 0.9346\n",
      "Epoch 140/500\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.2450 - categorical_accuracy: 0.9421\n",
      "Epoch 141/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2403 - categorical_accuracy: 0.9427\n",
      "Epoch 142/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.3492 - categorical_accuracy: 0.9216\n",
      "Epoch 143/500\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.2409 - categorical_accuracy: 0.9423\n",
      "Epoch 144/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2288 - categorical_accuracy: 0.9460\n",
      "Epoch 145/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2268 - categorical_accuracy: 0.9463\n",
      "Epoch 146/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2264 - categorical_accuracy: 0.9463\n",
      "Epoch 147/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2263 - categorical_accuracy: 0.9463\n",
      "Epoch 148/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2262 - categorical_accuracy: 0.9463\n",
      "Epoch 149/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2260 - categorical_accuracy: 0.9463\n",
      "Epoch 150/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2258 - categorical_accuracy: 0.9463\n",
      "Epoch 151/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2258 - categorical_accuracy: 0.9463\n",
      "Epoch 152/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2257 - categorical_accuracy: 0.9463\n",
      "Epoch 153/500\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.2258 - categorical_accuracy: 0.9463\n",
      "Epoch 154/500\n",
      "290/290 [==============================] - 4s 15ms/step - loss: 0.2257 - categorical_accuracy: 0.9463\n",
      "Epoch 155/500\n",
      "290/290 [==============================] - 5s 17ms/step - loss: 0.2257 - categorical_accuracy: 0.9463\n",
      "Epoch 156/500\n",
      "290/290 [==============================] - 6s 19ms/step - loss: 0.2275 - categorical_accuracy: 0.9461\n",
      "Epoch 157/500\n",
      "290/290 [==============================] - 4s 15ms/step - loss: 0.4965 - categorical_accuracy: 0.9001\n",
      "Epoch 158/500\n",
      "290/290 [==============================] - 4s 15ms/step - loss: 0.2724 - categorical_accuracy: 0.9332\n",
      "Epoch 159/500\n",
      "290/290 [==============================] - 5s 16ms/step - loss: 0.3076 - categorical_accuracy: 0.9274\n",
      "Epoch 160/500\n",
      "290/290 [==============================] - 5s 16ms/step - loss: 0.2359 - categorical_accuracy: 0.9445\n",
      "Epoch 161/500\n",
      " 60/290 [=====>........................] - ETA: 3s - loss: 0.2555 - categorical_accuracy: 0.9411"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-e5a858c63963>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmycallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\pygpu22\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pygpu22\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pygpu22\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \"\"\"\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pygpu22\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pygpu22\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pygpu22\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    343\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m           \u001b[0mnumpy_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pygpu22\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pygpu22\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pygpu22\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pygpu22\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pygpu22\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \"\"\"\n\u001b[0;32m   1062\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pygpu22\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1027\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " model.fit(X_train, y_train, epochs=500, callbacks=mycallbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92e50d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 15, 1024)          130048    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15, 512)           524800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15, 256)           131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15, 128)           32896     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 15, 64)            8256      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 960)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 29)                27869     \n",
      "=================================================================\n",
      "Total params: 855,197\n",
      "Trainable params: 855,197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d7500",
   "metadata": {},
   "source": [
    "# 13. Make prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad0b5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a876184e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'space'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories[np.argmax(res[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc1c899b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'space'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories[np.argmax(y_test[2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6514731",
   "metadata": {},
   "source": [
    "# 14. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d504a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model27improved2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec311a7",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d83af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model27improved2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0bd42",
   "metadata": {},
   "source": [
    "# 15. Evaluation using Confusion Matrix and Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6957b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df109360",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5506f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83e97d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2226,    0],\n",
       "        [   8,   86]],\n",
       "\n",
       "       [[2243,    0],\n",
       "        [   1,   76]],\n",
       "\n",
       "       [[2240,    2],\n",
       "        [   5,   73]],\n",
       "\n",
       "       [[2226,    0],\n",
       "        [   1,   93]],\n",
       "\n",
       "       [[2261,    1],\n",
       "        [   3,   55]],\n",
       "\n",
       "       [[2235,    0],\n",
       "        [   0,   85]],\n",
       "\n",
       "       [[2260,    0],\n",
       "        [   0,   60]],\n",
       "\n",
       "       [[2240,    2],\n",
       "        [   1,   77]],\n",
       "\n",
       "       [[2242,    4],\n",
       "        [   8,   66]],\n",
       "\n",
       "       [[2232,    1],\n",
       "        [   9,   78]],\n",
       "\n",
       "       [[2244,    4],\n",
       "        [   1,   71]],\n",
       "\n",
       "       [[2233,    0],\n",
       "        [   0,   87]],\n",
       "\n",
       "       [[2225,    9],\n",
       "        [  11,   75]],\n",
       "\n",
       "       [[2241,    4],\n",
       "        [  15,   60]],\n",
       "\n",
       "       [[2227,    0],\n",
       "        [   9,   84]],\n",
       "\n",
       "       [[2242,    0],\n",
       "        [  13,   65]],\n",
       "\n",
       "       [[2249,    0],\n",
       "        [   4,   67]],\n",
       "\n",
       "       [[2225,    2],\n",
       "        [  18,   75]],\n",
       "\n",
       "       [[2242,    5],\n",
       "        [   6,   67]],\n",
       "\n",
       "       [[2238,    0],\n",
       "        [   1,   81]],\n",
       "\n",
       "       [[2231,   12],\n",
       "        [  11,   66]],\n",
       "\n",
       "       [[2230,    4],\n",
       "        [   3,   83]],\n",
       "\n",
       "       [[2243,    0],\n",
       "        [   2,   75]],\n",
       "\n",
       "       [[2245,    1],\n",
       "        [  11,   63]],\n",
       "\n",
       "       [[2227,    0],\n",
       "        [   4,   89]],\n",
       "\n",
       "       [[2225,    1],\n",
       "        [   6,   88]],\n",
       "\n",
       "       [[2247,    0],\n",
       "        [  10,   63]],\n",
       "\n",
       "       [[2232,    4],\n",
       "        [  14,   70]],\n",
       "\n",
       "       [[2134,  119],\n",
       "        [   0,   67]]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "629f72b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9245689655172413"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f11bf8",
   "metadata": {},
   "source": [
    "# 16. Testing in real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89be8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245),(166,107,245),(106,167,245),(106,117,205),(161,107,25),(130,187,245),(106,117,205),(106,100,245),(196,107,245), (117,245,16), (16,117,245),(245,117,16), (117,245,16), (16,117,245),(166,107,245),(106,167,245),(106,117,205),(161,107,25),(130,187,245),(106,117,205),(106,100,245),(196,107,245), (117,245,16), (16,117,245),(245,117,16), (117,245,16), (16,117,245)]\n",
    "# def prob_viz(res, actions, input_frame, colors):\n",
    "#     output_frame = input_frame.copy()\n",
    "#     for num, prob in enumerate(res):\n",
    "#         if prob > 90:\n",
    "#             cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "#             cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2, cv2.LINE_AA)\n",
    "#     return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5b7b502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_viz(res, actions,input_frame):\n",
    "    output_frame = input_frame.copy()\n",
    "    if res[np.argmax(res)] > 0.9:\n",
    "        cv2.putText(output_frame,actions[np.argmax(res)], (200,300), cv2.FONT_HERSHEY_SIMPLEX,4, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c7b5ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "back = cv2.imread('board1.jpg')\n",
    "back = cv2.resize(back,(600,600))\n",
    "# cv2.imshow('back',back)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3dbae4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoriesLatest = np.array(['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','d',' ','n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "be21bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. New detection variables mode1\n",
    "# import time\n",
    "# sequence = []\n",
    "# sentence = []\n",
    "# threshold = 0.9\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# started = False\n",
    "# # Set mediapipe model \n",
    "# with mpHands.Hands() as hand:\n",
    "#     i = 20\n",
    "#     j = 26\n",
    "#     back2 = cv2.imread('board1.jpg')\n",
    "#     back2 = cv2.resize(back,(600,600))\n",
    "#     while cap.isOpened():\n",
    "#         current_time = time.time()\n",
    "#         board = back\n",
    "#         # Read feed\n",
    "#         ret, frame = cap.read()\n",
    "#         # Make detections\n",
    "#         image, results = mediapipe_detection(frame,hand)\n",
    "\n",
    "        \n",
    "#         # Draw landmarks\n",
    "#         draw_landmarks(image, results)\n",
    "#         # 2. Prediction logic\n",
    "#         keypoints = extract_keypoints(results)\n",
    "\n",
    "#         sequence.append(keypoints)\n",
    "# #         print(sequence)\n",
    "#         sequence = sequence[-15:]\n",
    "# #         print(sequence)\n",
    "        \n",
    "#         if len(sequence) == 15:\n",
    "#             res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "#         #3. Viz logic\n",
    "#             if res[np.argmax(res)] > threshold: \n",
    "#                 casl = categoriesLatest[np.argmax(res)]\n",
    "#                 if len(sentence) <= 0 or ( casl != 'n' and casl != sentence[-1]):\n",
    "#                     cv2.putText(back2,casl, (i,j), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "#                     if i <= 20 and  started:\n",
    "#                         j = j + 26\n",
    "#                     i = (i + 20)%500\n",
    "#                     if i == 0:\n",
    "#                         i = 20\n",
    "#                     started = True\n",
    "#                 if len(sentence) > 0: \n",
    "#                     if categories[np.argmax(res)] != sentence[-1]:\n",
    "#                         sentence.append(categoriesLatest[np.argmax(res)])\n",
    "#                 else:\n",
    "#                     sentence.append(categoriesLatest[np.argmax(res)]) \n",
    "\n",
    "#             if len(sentence) > 10: \n",
    "#                 sentence = sentence[-20:]\n",
    "\n",
    "#             # Viz probabilities\n",
    "#             board = prob_viz(res, categories,board)\n",
    "            \n",
    "        \n",
    "#         # Show to screen\n",
    "#         cv2.imshow('OpenCV Feed', image)\n",
    "#         cv2.imshow('viz',board)\n",
    "#         cv2.imshow('Text Converter',back2)\n",
    "\n",
    "#         # Break gracefully\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86a12840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables mode 2\n",
    "import time\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.9\n",
    "time_frame = []\n",
    "cap = cv2.VideoCapture(0)\n",
    "started = False\n",
    "# Set mediapipe model \n",
    "with mpHands.Hands() as hand:\n",
    "    i = 20\n",
    "    j = 26\n",
    "    back2 = cv2.imread('board1.jpg')\n",
    "    back2 = cv2.resize(back,(600,600))\n",
    "    while cap.isOpened():\n",
    "        \n",
    "        board = back\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame,hand)\n",
    "\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "\n",
    "        sequence.append(keypoints)\n",
    "#         print(sequence)\n",
    "        sequence = sequence[-15:]\n",
    "#         print(sequence)\n",
    "        \n",
    "        if len(sequence) == 15:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                casl = categoriesLatest[np.argmax(res)]\n",
    "                if casl != 'n' and (cv2.waitKey(1) & 0xFF == ord('v')) :\n",
    "                    cv2.putText(back2,casl, (i,j), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                    if i <= 20 and  started:\n",
    "                        j = j + 26\n",
    "                    i = (i + 20)%500\n",
    "                    if i == 0:\n",
    "                        i = 20\n",
    "                    started = True\n",
    "\n",
    "            # Viz probabilities\n",
    "            board = prob_viz(res, categories,board)\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        cv2.imshow('viz',board)\n",
    "        cv2.imshow('Text Converter',back2)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c3bc19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-58691b4d5265>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.90\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mstarted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Set mediapipe model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# # 1. New detection variables\n",
    "\n",
    "# sequence = []\n",
    "# sentence = []\n",
    "# threshold = 0.90\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# started = False\n",
    "# # Set mediapipe model\n",
    "# pred_count = {'count':0,'symbol':'n'}\n",
    "# prev_symbol = 'n'\n",
    "# with mpHands.Hands() as hand:\n",
    "#     i = 20\n",
    "#     j = 26\n",
    "#     back2 = cv2.imread('board1.jpg')\n",
    "#     back2 = cv2.resize(back,(600,600))\n",
    "#     while cap.isOpened():\n",
    "        \n",
    "#         board = back\n",
    "#         # Read feed\n",
    "#         ret, frame = cap.read()\n",
    "#         # Make detections\n",
    "#         image, results = mediapipe_detection(frame,hand)\n",
    "\n",
    "        \n",
    "#         # Draw landmarks\n",
    "#         draw_landmarks(image, results)\n",
    "#         # 2. Prediction logic\n",
    "#         keypoints = extract_keypoints(results)\n",
    "\n",
    "#         sequence.append(keypoints)\n",
    "# #         print(sequence)\n",
    "#         sequence = sequence[-15:]\n",
    "# #         print(sequence)\n",
    "        \n",
    "#         if len(sequence) == 15:\n",
    "#             res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "#         #3. Viz logic\n",
    "#             if res[np.argmax(res)] > threshold: \n",
    "#                 casl = categoriesLatest[np.argmax(res)]\n",
    "#                 if casl == pred_count['symbol']:\n",
    "#                     pred_count['count'] += 1\n",
    "#                 else:\n",
    "#                     pred_count['symbol'] = casl\n",
    "#                     pred_count['count'] = 0\n",
    "#                 if ( casl != 'n'  and pred_count['count'] >= 10 ):\n",
    "#                     cv2.putText(back2,casl, (i,j), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "#                     if i <= 20 and  started:\n",
    "#                         j = j + 26\n",
    "#                     i = (i + 20)%500\n",
    "#                     if i == 0:\n",
    "#                         i = 20\n",
    "#                     started = True\n",
    "#                     pred_count['count'] = 0\n",
    "\n",
    "#             print(pred_count)\n",
    "\n",
    "\n",
    "#             # Viz probabilities\n",
    "#             board = prob_viz(res, categories,board, colors)\n",
    "            \n",
    "\n",
    "        \n",
    "#         # Show to screen\n",
    "#         cv2.imshow('OpenCV Feed', image)\n",
    "#         cv2.imshow('viz',board)\n",
    "#         cv2.imshow('Text Converter',back2)\n",
    "\n",
    "#         # Break gracefully\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8f6c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936ae17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
